---
title: "Consolidation des connaissances -- les r√©seaux lexicaux"
author: "Pascal Brissette"
date: 2025-09-13
format:
  html:
    toc: true
    toc-depth: 2
    toc-location: left
    code-fold: true
    code-tools: true
    theme: Yeti
    reference-location: margin
    title-block-banner: true
    self-contained: true
    standalone: true
editor: visual
lang: fr
execute:
  freeze: auto
  echo: true
  warning: false
  message: false
---

```{r, include=FALSE}

install_and_load <- function(packages) {
  for (pkg in packages) {
    if (!pkg %in% rownames(installed.packages())) {
      install.packages(pkg)
    }
    library(pkg, character.only = TRUE)
  }
}

pkgs <- c(
  "data.table",
  "tidyr",
  "stringr",
  "kableExtra",
  "DT",
  "gutenbergr",
  "dplyr",
  "igraph",
  "ggplot2",
  "readr"
)

install_and_load(pkgs)
```

On l'a dit d√©j√† : l'analyse de r√©seau est une m√©thode extr√™mement flexible qui peut s'adapter √† des types d'entit√©s (ex. : √™tres humains, personnages de fiction, villes, prot√©ines, v√©hicules) et des types de relation aussi diverses (ex. : communication par lettre ou pr√©sence dans un m√™me espace de deux individus ou personnages, flux migratoires, co-expression, mobilit√©). M√™me si on cible le seul domaine des √©tudes litt√©raires, la vari√©t√© des usages de l'analyse de r√©seau est impressionnante : cartographie de personnages, structure narrative, intertextualit√©, √©tudes th√©matiques, histoire litt√©raire. **D√®s qu'il s'agit d'√©tudier des relations, des interactions, des d√©placements, des cooccurrences, et de mesurer la force et, si on le souhaite, la direction de relations entre des entit√©s, la th√©orie des r√©seaux constitue une alli√©e de premier plan**.

Apr√®s avoir explor√© les r√©seaux de personnages de *La Com√©die humaine* et d'*Illusions perdues*, nous allons maintenant changer de perspective pour examiner les mots eux-m√™mes comme acteurs du r√©seau. Car, oui, m√™me des mots peuvent devenir les entit√©s d'un r√©seau! Pour consolider notre ma√Ætrise de l'analyse de r√©seau tout en approfondissant notre compr√©hension d'*Illusions perdues*, nous allons proc√©der √† une analyse de son lexique, ce qui nous permettra d'observer l'√©volution th√©matique √† travers les trois parties du roman.

## Description du projet

Ce projet vise √† analyser le vocabulaire des trois parties du roman *Illusions perdues* pour y d√©celer les th√®mes. Notre premi√®re √©tape consistera donc √† d√©couper le texte en trois parties (¬´Les deux po√®tes¬ª, ¬´Un grand homme de province √† Paris¬ª, ¬´Les souffrances de l'inventeur¬ª).

Ensuite, nous filtrerons le vocabulaire du roman pour ne conserver que les mots porteurs de sens. Parmi les nombreuses m√©thodes de filtrage, nous utilerons celle qui consiste √† ne retenir que certaines cat√©gories grammaticales : les noms communs, les noms propres, les adjectifs et les verbes. Allons-nous devoir analyser chaque phrase manuellement pour en extraire les mots correspondant √† ces cat√©gories ? Heureusement, non ! Nous emploierons un mod√®le de langage sp√©cialis√© qui, pour chaque mot du roman, apposera une √©tiquette grammaticale (appel√©e *Part-of-Speech tag* ou `POS`). Nous pourrons ensuite utiliser ces √©tiquettes pour extraire les mots ¬´pleins¬ª du roman.

Une fois le vocabulaire filtr√©, nous construirons un **r√©seau**. Les mots s√©lectionn√©s deviendront les **sommets** de ce r√©seau, et leur **cooccurrence** au sein d'une m√™me phrase formera les **liens** qui les relient. Nous obtiendrons ainsi une carte visuelle des relations entre les concepts, les personnages et les actions du roman.

## Pr√©paration du corpus

La pr√©paration du corpus comprend l'importation du texte depuis le site du Projet Gutenberg, l'√©limination du paratexte √©ditorial (ce sont des mots qu'on ne veut pas voir dans notre r√©seau!), l'identification des fronti√®res entre les parties, et finalement la cr√©ation d'une structure de donn√©es qui associe chaque mot √† sa partie d'origine.

```{r load-text}
library(gutenbergr)

# 1. Chargement du texte depuis Gutenberg
text_gutenberg <- gutenberg_download("54723", mirror = "http://aleph.gutenberg.org")

# 2. Nettoyage : Suppression de l'en-t√™te et du pied de page de Gutenberg
start_line <- which(str_detect(text_gutenberg$text, "A MONSIEUR VICTOR HUGO."))
end_line <- which(str_detect(text_gutenberg$text, "FIN DU HUITI√àME VOLUME."))
text_body <- text_gutenberg |> 
  slice(start_line:end_line)

# 3. Identification des d√©buts de parties
premiere_partie_start <- which(str_detect(text_body$text, "PREMI√àRE PARTIE."))
deuxieme_partie_start <- which(str_detect(text_body$text, "DEUXI√àME PARTIE."))
troisieme_partie_start <- which(str_detect(text_body$text, "TROISI√àME PARTIE."))

# 4. Attribution des parties √† chaque ligne
text_with_parts <- text_body |> 
  mutate(
    ligne = row_number(),
    partie = case_when(
      ligne >= premiere_partie_start & ligne < deuxieme_partie_start ~ "Partie_1",
      ligne >= deuxieme_partie_start & ligne < troisieme_partie_start ~ "Partie_2",
      ligne >= troisieme_partie_start ~ "Partie_3",
      TRUE ~ "Avant_roman"  # Pour les lignes avant la premi√®re partie
    )
  )

# 5. Cr√©ation des paragraphes avec identification de partie
paragraphs_data <- text_with_parts |> 
  filter(partie != "Avant_roman") |> 
  # Cr√©ation d'un ID de paragraphe qui se r√©initialise pour chaque partie
  group_by(partie) |>
  mutate(
    paragraph_id_in_part = cumsum(text == "" | str_detect(text, "^\\s*$")) + 1
  ) |>
  ungroup() |>
  filter(text != "", !str_detect(text, "^\\s*$")) |> 
  group_by(partie, paragraph_id_in_part) |> 
  summarise(
    paragraph_text = paste(text, collapse = " "),
    .groups = "drop"
  ) |>
  # Cr√©ation d'un identifiant unique de document pour chaque paragraphe
  mutate(doc_id = paste(partie, paragraph_id_in_part, sep = "_")) |> 
  select(doc_id, partie, paragraph_id_in_part, paragraph_text)

paragraphs_data |> head()

```

## Pause m√©thodologique : la loi de Zipf

Avant de proc√©der au nettoyage lexical, prenons un moment pour explorer une propri√©t√© fondamentale des corpus textuels : la **loi de Zipf**. Cette loi empirique, formul√©e par le linguiste George Kingsley Zipf en 1949, stipule que dans un corpus textuel, la fr√©quence d'un mot est inversement proportionnelle √† son rang dans la liste des mots class√©s par fr√©quence d√©croissante. Pour le dire de mani√®re simple: **une petite minorit√© de mots appara√Æt tr√®s souvent, tandis que la grande majorit√© des mots n'appara√Æt que rarement**. Concr√®tement, dans *Illusions perdues*, les 100 mots les plus fr√©quents repr√©sentent probablement plus de la moiti√© du texte total !

Cette distribution n'est pas un hasard : elle refl√®te la structure m√™me du langage. Les mots les plus fr√©quents sont g√©n√©ralement des chevilles grammaticales ("le", "de", "et", "dans", "un") qui structurent les phrases mais portent peu d'information s√©mantique. √Ä l'autre extr√™me, nous trouvons de nombreux mots qui n'apparaissent qu'une seule fois dans tout le roman (les *hapax*).

V√©rifions cette loi sur notre corpus d'*Illusions perdues* :

```{r loi de Zipf}

# Cr√©ation d'un corpus simple pour analyser les fr√©quences
corpus_simple <- paragraphs_data |>
  # Tokenisation basique
  mutate(text_clean = str_to_lower(paragraph_text)) |>
  mutate(text_clean = str_remove_all(text_clean, "[[:punct:]]")) |>
  separate_rows(text_clean, sep = "\\s+") |>
  filter(text_clean != "") |>
  rename(mot = text_clean)

# Calcul des fr√©quences et rangs
frequences_zipf <- corpus_simple |>
  count(mot, sort = TRUE) |>
  mutate(
    rang = row_number(),
    log_rang = log10(rang),
    log_freq = log10(n)
  )

# Activer ggplot2 si cela n'a pas d√©j√† √©t√© fait
library(ggplot2)

# Graphique en √©chelle logarithmique
p1 <- frequences_zipf |>
  head(500) |> 
  ggplot(aes(x = rang, y = n)) + # <-- ICI : rang en x, fr√©quence (n) en y
  geom_point(alpha = 0.6, color = "steelblue") +
  labs(
    title = "Loi de Zipf dans Illusions perdues",
    subtitle = "Chute de la fr√©quence en fonction du rang",
    x = "Rang du mot (par fr√©quence d√©croissante)", # label corrig√©
    y = "Fr√©quence totale (nombre d'occurrences)"  # label corrig√©
  ) +
  theme_minimal()

p1

# Affichage des mots les plus fr√©quents
frequences_zipf |>
  head(10) |>
  select(mot, n, rang) |>
  print()
```

## Cons√©quences pratiques pour l'analyse de r√©seau

Le graphique et la liste de mots ci-dessus ne sont pas de simples illustrations ; ils sont un avertissement. Ils nous montrent pr√©cis√©ment ce qui se passerait si nous construisions notre r√©seau lexical √† partir du texte brut : apr√®s le travail du dernier cours, vous le devinez, le r√©sultat serait un graphe illisible, une immense "boule de poils".

La table des 10 mots les plus fr√©quents le confirme : les sommets les plus importants de ce r√©seau ne seraient pas ¬´lucien¬ª ou ¬´Bargeton¬ª ou ¬´amour¬ª, mais les chevilles grammaticales ¬´de¬ª, ¬´la¬ª, ¬´√†¬ª, ¬´et¬ª, ¬´il¬ª... L'√©limination de ces mots vides (en anglais: *stopwords*), s√©mantiquement pauvres, est donc notre premi√®re t√¢che pour faire √©merger un signal pertinent.

Mais ce n'est que la premi√®re √©tape. Le d√©fi suivant est plus cors√© et concerne la richesse de la langue fran√ßaise elle-m√™me. Lorsque nous lisons ¬´pensa¬ª, ¬´pensait¬ª, ¬´pense¬ª et ¬´penser¬ª, notre cerveau les identifie comme un seul et m√™me concept: l'acte de ¬´penser¬ª. L'ordinateur, lui, voit quatre formes distinctes et ne va pas ¬´penser¬ª (!) √† les regrouper, √† moins que ne lui commande. Si nous laissons les choses en l'√©tat, nous allons surcharger notre graphe d'une multitude de n≈ìuds redondants et diluer la fr√©quence r√©elle de chaque concept. Pour contrer cet effet, nous devons ramener toutes ces variations √† une forme unique, leur **lemme** (la forme du dictionnaire). C'est seulement ainsi que nous obtiendrons la v√©ritable fr√©quence du verbe ¬´penser¬ª et que ce concept pourra peser de tout son poids dans notre r√©seau.

### Annotation automatique et utilisation des lemmes

Pour palier les probl√®mes expos√©s ci-dessus, nous allons tout d'abord ex√©cuter une annotation morphosyntaxique de notre texte. Ce type d'annotation, men√©e √† l'aide d'un mod√®le de langage (non g√©n√©ratif), segmente le texte en paragraphes et en phrases, analyse chaque mot de chaque phrase, d√©termine sa nature et sa fonction, puis, partant de l√†, d√©termine sa forme neutre, son lemme. Le ¬´lemme¬ª est la forme canonique ou non fl√©chie d'un mot, telle qu'elle apparait dans un dictionnaire de langue. Ainsi, ¬´penser¬ª est le lemme unique des quatre formes fl√©chies donn√©es en exemple ci-dessus. Ce sont ces lemmes que nous allons utiliser dans notre r√©seau lexical, de mani√®re √† r√©duire le nombre de sommets.

> üîé L'**annotation automatique** consiste √† utiliser des outils informatiques (algorithmes, r√©seaux de neurones, IA g√©n√©rative, etc.) pour enrichir automatiquement des donn√©es textuelles avec des informations linguistiques structur√©es. Dans le contexte du Traitement automatique des langues (TAL), l'annotation automatique consiste le plus souvent √† ajouter des √©tiquettes (morphosyntaxiques, s√©mantiques, pragmatique, etc.) √† des mots, voire √† tous les mots d'un texte.
>
> üîé L'**annotation morphosyntaxique** (ou *POS Tagging* en anglais) est un type d'annotation automatique. Le processus consiste √† assigner des informations morphologiques et syntaxiques √† chaque mot d'un texte. Cette t√¢che est ex√©cut√©e √† l'aide d'un r√©seau de neurones pr√©-entrain√© sur de grandes masses de textes.
>
> üîé Le **lemme** est la forme canonique ou "forme du dictionnaire" d'un mot. Le processus de *lemmatisation* consiste √† r√©duire toutes les variantes flexionnelles d'un mot (conjugaisons, accords en genre et en nombre) √† cette forme de base unique.

### **Filtrage par cat√©gorie grammaticale**

L'annotation morphosyntaxique nous permet d'exploiter deux niveaux d'information linguistique : d'une part les **lemmes**, d'autre part les **√©tiquettes morphosyntaxiques** qui identifient la cat√©gorie grammaticale et les traits morphologiques de chaque mot (en anglais: *token*).

**Mise en ≈ìuvre technique**

Le pipeline d'annotation s'appuie sur le mod√®le **french-gsd** d'UDPipe, entra√Æn√© sur le corpus *Universal Dependencies* pour le fran√ßais. Le processus comprend plusieurs √©tapes automatis√©es :

1.  **Segmentation** : d√©coupage du texte brut en phrases puis en tokens

2.  **Annotation morphosyntaxique** : attribution d'une √©tiquette POS √† chaque token

3.  **Lemmatisation** : normalisation de chaque forme fl√©chie vers sa forme canonique

4.  **Filtrage** : s√©lection des tokens selon les crit√®res cat√©goriels d√©finis

UDPipe traite l'ensemble du corpus en une seule passe, garantissant la coh√©rence des annotations sur tous les paragraphes.

Note: comme l'annotation automatique peut prendre plusieurs minutes, le texte a √©t√© pr√©-annot√© et enregistr√© dans votre dossier `donnees`, d'o√π vous pourrez l'importer en ex√©cutant le bloc suivant.

```{r}
# library(udpipe)
# # 1. T√©l√©chargement du mod√®le de langue fran√ßaise (si non pr√©sent)
# ud_model_path <- "french-gsd-ud-2.5-191206.udpipe"
# if (!file.exists(ud_model_path)) {
#   udpipe_download_model(language = "french-gsd", model_dir = getwd())
# }
# 
# # 2. Chargement du mod√®le
# ud_model <- udpipe_load_model(file = ud_model_path)
# 
# # 3. Annotation du texte de nos paragraphes
# # On utilise les colonnes 'doc_id' et 'paragraph_text' de notre table 'paragraphs_data'
# annotations <- udpipe_annotate(ud_model, 
#                                x = paragraphs_data$paragraph_text,
#                                doc_id = paragraphs_data$doc_id)
# 
# # 4. Conversion du r√©sultat en un format 'tibble' pour une manipulation ais√©e
# annotations_df <- as_tibble(annotations)
# 
# # 5. Rajout de l'information 'partie' dans notre table d'annotations
# # On extrait la partie depuis le doc_id (ex: "Partie_1_1" -> "Partie_1")
# annotations_df <- annotations_df |>
#   mutate(partie = str_extract(doc_id, "Partie_[1-3]"))
# 
# saveRDS(annotations_df, "donnees/annotations_df.RDS")

annotations_df <- readRDS("donnees/annotations_df.RDS")

```

### Exploration du r√©sultat de l'annotation

Jetons un ≈ìil au tableau de donn√©es que nous venons de cr√©er. Chaque ligne correspond √† un mot (*token*) du roman. Pour chaque mot, des informations sur sa position dans la phrase, sa cat√©gorie grammaticale et sa morphologie sont fournies.

```{r}
library(kableExtra)

annotations_df[8:25,] |>
  #select(doc_id, token, lemma, upos, partie) |>
  #head(13) |>
  kableExtra::kbl(caption = "Aper√ßu des donn√©es annot√©es avec udpipe") |>
  kableExtra::kable_styling(bootstrap_options = "striped", full_width = F)
```

Nous voyons par exemple que les mots ¬´l'¬ª et ¬´√©poque¬ª ont √©t√© correctement identifi√©s comme d√©terminant (DET) et nom (NOUN), et que leur lemme est ¬´le¬ª et ¬´√©poque¬ª.

## Cr√©ation des donn√©es du r√©seau : n≈ìuds et liens

Nous allons maintenant sculpter la petite base de donn√©es retourn√©es par UDPipe pour en extraire les deux fichiers n√©cessaires √† toute analyse de r√©seau dans Gephi :

-   Une table des n≈ìuds (ou sommets), qui liste les entit√©s de notre r√©seau.
-   Une table des liens (ou ar√™tes), qui d√©crit les relations entre ces entit√©s.

### **1. S√©lection des n≈ìuds : filtrage par cat√©gorie grammaticale**

Notre objectif est de cartographier la structure conceptuelle et th√©matique du roman. Les pronoms (`il`, `elle`), les d√©terminants (`le`, `un`) ou les pr√©positions (`√†`, `dans`) sont essentiels √† la syntaxe, mais ils ne contribuent pas √† la formation de th√®mes. Nous allons donc d√©cider que nos n≈ìuds seront les lemmes relevant uniquement des **cat√©gories grammaticales** suivantes :

-   `NOUN` : noms communs (ex: `amour`, `argent`, `art`)

-   `PROPN` : noms propres (ex: `Lucien`, `Paris`, `David`)

-   `ADJ` : adjectifs (ex: `beau`, `pauvre`, `grand`)

-   `VERB` : verbes (ex: `aimer`, `√©crire`, `mourir`)

De plus, la loi de Zipf nous a montr√© qu'un tr√®s grand nombre de mots n'apparaissent qu'une seule fois dans tout le roman (les hapax). Ces lemmes tr√®s rares ont peu de chance de former des connexions significatives et peuvent encombrer notre graphe. Nous allons donc √©galement les retirer.

Enfin, pour r√©duire encore la taille de nos tables, nous allons filtrer les sommets (lemmes) par seuil de fr√©quence (sup√©rieure √† 10 occurrences dans le texte) et supprimer les derniers mots trop communs √† l'aide d'un antidictionnaire.

Le bloc ci-dessous impl√©mente ces √©tapes.

```{r}

library(lsa)
# D√©finition des cat√©gories grammaticales qui nous int√©ressent
pos_of_interest <- c("NOUN", "PROPN", "ADJ", "VERB")

# Filtrage des annotations pour ne garder que les lemmes pertinents
lemmes_interessants <- annotations_df |>
  filter(upos %in% pos_of_interest)

nodes_table_auto <- lemmes_interessants |>
  group_by(lemma) |>
  summarise(
    Total_Frequency = n(),
    POS_Category = names(which.max(table(upos)))
  ) |>
  ungroup()

nodes_table_auto <- nodes_table_auto |> filter(!lemma %in% c("voici",
                                         "voil√†",
                                         "ser",
                                         "riir",
                                         "ir",
                                         "ter",
                                         "sere",
                                         "-A",
                                         "il",
                                         "ire",
                                         "lure",
                                         "donn",
                                         "son",
                                         "adieu"))

library(data.table)
setDT(nodes_table_auto)

nodes_table_auto[lemma == "cavalier", POS_Category:="NOUN"]


nodes_table <- nodes_table_auto |>
  # La suite du processus est identique
  filter(Total_Frequency > 15) |>
  rename(Id = lemma) |>
  mutate(Label = Id) |>
  select(Id, Label, POS_Category, Total_Frequency) |>
  arrange(desc(Total_Frequency)) |> 
  # 6. Supprimer les mots trop communs (ex: "avoir", "dire")
  filter(!Label %in% lsa::stopwords_fr)

# Affichage des premi√®res lignes de notre table de n≈ìuds
cat("Aper√ßu de la table des n≈ìuds (sommets) :\n")
head(nodes_table) |> print()

cat(paste("\nNombre total de n≈ìuds apr√®s filtrage :", nrow(nodes_table), "\n"))
```

Nous sommes pass√©s de 12 343 lemmes distincts √† 1 134, tout en conservant le c≈ìur s√©mantique du texte. Chaque lemme est maintenant un n≈ìud potentiel de notre r√©seau.

### **2. D√©finition des liens : la cooccurrence par phrase**

Un lien (ou une ar√™te) existera entre deux lemmes s'ils apparaissent ensemble (**cooccurre**) dans la m√™me phrase.

> üîé La **cooccurrence** d√©signe le fait que deux ou plusieurs mots apparaissent ensemble √† proximit√© dans un texte. Pour que ce concept soit op√©rationnel, nous devons d√©finir cette ¬´proximit√©¬ª en choisissant une ¬´fen√™tre de contexte¬ª.

Pour suivre l'√©volution th√©matique du roman, nous ne voulons pas seulement savoir *si* deux mots apparaissent ensemble, mais √©galement *dans quelle partie du roman* cette cooccurrence a lieu.

Comme discut√© pr√©c√©demment, nous allons construire une table de liens qui sp√©cifie non seulement les deux n≈ìuds connect√©s (`Source`, `Target`) et le poids de leur lien, mais aussi la `Partie` o√π ce lien a √©t√© observ√©.

Le processus est le suivant :

1.  On ne garde que les lemmes qui ont pass√© notre filtre pr√©c√©dent (pr√©sents dans `nodes_table`).

2.  Pour chaque phrase de chaque partie, on cr√©e toutes les paires de lemmes possibles.

3.  On compte combien de fois chaque paire appara√Æt dans chaque partie du roman. Ce nombre sera le poids (`Weight`) du lien.

```{r}
# On ne garde que les annotations correspondant √† nos n≈ìuds s√©lectionn√©s
annotations_filtered <- lemmes_interessants |>
  filter(lemma %in% nodes_table$Id)

# Pour cr√©er les paires, on joint la table √† elle-m√™me
# On prend tous les mots d'une m√™me phrase et on les associe
# La condition `lemma.x < lemma.y` √©vite les doublons (a,b)/(b,a) et les boucles (a,a)
paired_lemmes <- annotations_filtered |>
  select(doc_id, sentence_id, lemma, partie) |>
  inner_join(
    annotations_filtered |>
      select(doc_id, sentence_id, lemma),
    by = c("doc_id", "sentence_id"),
    relationship = "many-to-many"
  ) |>
  filter(lemma.x < lemma.y)

# Cr√©ation de la table des liens (ar√™tes)
edges_table <- paired_lemmes |>
  # 1. On groupe par paire de lemmes et par partie du roman
  group_by(Source = lemma.x, Target = lemma.y, Partie = partie) |>
  # 2. On compte le nombre de cooccurrences pour obtenir le poids
  summarise(Weight = n(), .groups = "drop") |>
  # 3. On s'assure d'avoir des colonnes bien nomm√©es et dans le bon ordre
  select(Source, Target, Weight, Partie) |>
  arrange(desc(Weight)) # On ordonne par poids d√©croissant

# Affichage des premi√®res lignes de notre table de liens
cat("Aper√ßu de la table des liens (ar√™tes) :\n")
head(edges_table) |> print()

cat(paste("\nNombre total de liens uniques (cooccurrence-partie) :", nrow(edges_table), "\n"))
```

Nous avons maintenant une table de liens parfaitement structur√©e. Par exemple, la premi√®re ligne nous indique combien de fois les lemmes ¬´madame¬ª et ¬´Bargeton¬ª apparaissent dans la m√™me phrase au sein de la ¬´Partie_1¬ª. Cette structure nous donnera la possibilit√© d'explorer le r√©seau global ou les sous-r√©seaux de chaque partie.

### **3. Sauvegarde les donn√©es pour Gephi**

La derni√®re √©tape dans R est de sauvegarder nos deux tables (`nodes_table` et `edges_table`) dans des fichiers au format `CSV`. Ces fichiers pourront √™tre directement import√©s dans Gephi.

```{r}
library(readr)
# Cr√©ation d'un dossier pour stocker les r√©sultats s'il n'existe pas
if (!dir.exists("Gephi")) {
  dir.create("Gephi")
}

# Sauvegarde des fichiers CSV
write_csv(nodes_table, "Gephi/illusions_perdues_lexique_nodes.csv")
write_csv(edges_table, "Gephi/illusions_perdues_lexique_edges.csv")

cat("Les fichiers 'illusions-perdues-nodes.csv' et 'illusions-perdues-edges.csv' ont √©t√© cr√©√©s dans le dossier 'gephi_data/'.\n")
```

Notre travail de pr√©paration des donn√©es est maintenant termin√©. Nous sommes pr√™ts √† passer √† la phase d'exploration et de visualisation.

Nous pouvons, comme nous l'avons fait pr√©c√©demment, importer les fichiers dans Gephi et explorer √† loisir le r√©seau avec les puissants outils disponibles.

Pour simplifier cette exploration, une application maison est mise √† votre disposition. En ex√©cutant le bloc de code ci-dessous, vous allez lancer l'application et pourrez interagir avec vos donn√©es sans sortir de Posit.

```{r}
shiny::runApp("Reseau_lexical/app.R")
```
